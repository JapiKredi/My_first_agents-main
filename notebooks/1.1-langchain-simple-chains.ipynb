{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (6.29.5)\n",
      "Requirement already satisfied: appnope in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (1.8.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (8.27.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: langchain-openai in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (0.2.13)\n",
      "Requirement already satisfied: langchain-core in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.55.3 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-openai) (1.58.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (4.66.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.7.24)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain langchain-openai langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_openai in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (0.2.13)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain_openai) (0.3.27)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.55.3 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain_openai) (1.58.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (0.2.4)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (4.66.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain_openai) (3.8)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain_openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_openai) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/vidhya-agents/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/vidhya-agents/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('/Users/jasper/Downloads/My_first_agents-main/notebooks/.env')\n",
    "\n",
    "# Verify that the API key is loaded\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Attention Mechanism**: Transformers use a method called \"attention\" to focus on different parts of the input data, allowing them to determine which words or elements are most important for understanding the context. This helps the model capture relationships between words in a sentence, regardless of their position.\n",
       "\n",
       "- **Parallel Processing**: Unlike older models that process information in sequence, transformers can analyze all parts of the input at once. This parallel processing makes them faster and more efficient, especially when handling large amounts of data.\n",
       "\n",
       "- **Layers and Encoding**: Transformers are built with multiple layers that transform the input data into a format the model can understand. Each layer refines the information, enabling the model to learn complex patterns and generate accurate outputs, like translations or text completions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', api_key=api_key)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a research assistant'),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "basic_chain = prompt | llm | output_parser\n",
    "\n",
    "output = basic_chain.invoke({'input': 'Write a 3 bullet point summary about how transformers work. Simplify to non-technical people but keep the main bits of information.'})\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a draft of a research report using chains in langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_SYS_MSG = \"\"\"\n",
    "You are a research assistant and a scientific writer.\n",
    "You take in requests about tpics and write organized research reprts on those topics.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', WRITER_SYS_MSG),\n",
    "    ('human', 'Write an organized research report about this topic:\\n\\n{topic}.')\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "writer_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Understanding Transformers: A Non-AI Researcher’s Guide\n",
       "\n",
       "## Introduction\n",
       "Transformers are a type of model architecture that has revolutionized the field of artificial intelligence (AI), particularly in natural language processing (NLP). Introduced in a 2017 paper titled \"Attention is All You Need\" by Vaswani et al., transformers have become the backbone of many state-of-the-art AI applications, including language translation, text generation, and more. This report aims to explain how transformers work in a straightforward manner, making it accessible for non-AI researchers.\n",
       "\n",
       "## 1. The Basics of Transformers\n",
       "\n",
       "### 1.1 What is a Transformer?\n",
       "A transformer is a neural network architecture designed to process sequential data, such as text. Unlike previous models that processed data in order (like recurrent neural networks), transformers can analyze all parts of the input simultaneously, which allows for greater efficiency and effectiveness.\n",
       "\n",
       "### 1.2 Key Components\n",
       "Transformers consist of several key components:\n",
       "- **Input Embeddings**: Words or tokens are converted into numerical vectors that represent their meanings.\n",
       "- **Positional Encoding**: Since transformers do not process data sequentially, positional encoding is added to the input embeddings to give the model information about the order of the words.\n",
       "- **Attention Mechanism**: This is the core innovation of transformers, allowing the model to focus on different parts of the input when making predictions.\n",
       "\n",
       "## 2. The Attention Mechanism\n",
       "\n",
       "### 2.1 What is Attention?\n",
       "The attention mechanism allows the model to weigh the importance of different words in a sentence when making predictions. For example, in the sentence \"The cat sat on the mat,\" the model can learn to focus more on \"cat\" and \"mat\" when predicting the next word.\n",
       "\n",
       "### 2.2 How Attention Works\n",
       "- **Query, Key, and Value**: Each word is transformed into three vectors: a query, a key, and a value. The query represents what the model is looking for, the key represents the information available, and the value is the actual information.\n",
       "- **Calculating Attention Scores**: The model calculates a score for how much focus to place on each word by taking the dot product of the query and key vectors. This score is then normalized using a softmax function to create a probability distribution.\n",
       "- **Weighted Sum**: Finally, the model computes a weighted sum of the value vectors based on the attention scores, allowing it to focus on the most relevant words.\n",
       "\n",
       "## 3. The Transformer Architecture\n",
       "\n",
       "### 3.1 Encoder and Decoder\n",
       "Transformers are typically composed of two main parts: the encoder and the decoder.\n",
       "- **Encoder**: The encoder processes the input data and generates a set of attention-based representations.\n",
       "- **Decoder**: The decoder takes these representations and generates the output, such as a translated sentence.\n",
       "\n",
       "### 3.2 Stacking Layers\n",
       "Both the encoder and decoder consist of multiple layers (often 6 to 12). Each layer contains:\n",
       "- **Multi-Head Attention**: This allows the model to focus on different parts of the input simultaneously.\n",
       "- **Feed-Forward Neural Networks**: These networks process the output from the attention mechanism.\n",
       "- **Residual Connections and Layer Normalization**: These techniques help stabilize training and improve performance.\n",
       "\n",
       "## 4. Training Transformers\n",
       "\n",
       "### 4.1 Data Preparation\n",
       "Transformers require large datasets for training. Text data is typically preprocessed to remove noise and convert words into tokens.\n",
       "\n",
       "### 4.2 Loss Function\n",
       "During training, the model's predictions are compared to the actual outputs using a loss function, which quantifies the difference. The goal is to minimize this loss through optimization techniques like gradient descent.\n",
       "\n",
       "### 4.3 Transfer Learning\n",
       "Once trained, transformers can be fine-tuned on specific tasks with smaller datasets, making them versatile for various applications.\n",
       "\n",
       "## 5. Applications of Transformers\n",
       "\n",
       "Transformers have a wide range of applications, including:\n",
       "- **Natural Language Processing**: Language translation, sentiment analysis, and text summarization.\n",
       "- **Computer Vision**: Image classification and object detection.\n",
       "- **Speech Recognition**: Converting spoken language into text.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Transformers represent a significant advancement in AI, particularly in how machines understand and generate human language. By leveraging the attention mechanism and a unique architecture, transformers can process information more effectively than previous models. Understanding the basics of transformers can provide valuable insights into the capabilities and potential applications of AI technologies in various fields. As research continues to evolve, transformers are likely to play an even more prominent role in shaping the future of AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = writer_chain.invoke({'topic': 'How do transformers work for non AI researchers?'})\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Clarity and Accessibility**: The report effectively breaks down complex concepts related to transformers into digestible sections, making it accessible for non-AI researchers. However, consider adding more analogies or real-world examples to further enhance understanding.\n",
       "\n",
       "- **Depth of Explanation**: While the report covers the basics well, it could benefit from a deeper exploration of the implications of the attention mechanism and how it differs from traditional methods. This would provide a more comprehensive understanding of why transformers are revolutionary.\n",
       "\n",
       "- **Visual Aids**: Incorporating diagrams or flowcharts to illustrate the transformer architecture and the attention mechanism would greatly enhance comprehension, especially for visual learners.\n",
       "\n",
       "- **Applications Section**: The applications of transformers are briefly mentioned but could be expanded with specific examples or case studies to illustrate their impact in various fields, particularly in NLP and computer vision.\n",
       "\n",
       "- **Future Directions**: The conclusion could be strengthened by discussing potential future developments in transformer research or applications, which would provide readers with insights into the evolving landscape of AI technologies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REVIEWER_SYS_MSG = \"\"\"\n",
    "You are a reviewer for research reports. You take in research reports and provide feecback on them.\n",
    "\"\"\"\n",
    "\n",
    "prompt_reviewer = ChatPromptTemplate.from_messages([\n",
    "    ('system', REVIEWER_SYS_MSG),\n",
    "    ('human', 'Provide feedback on this research report:\\n\\n{report}. As 5 concise bullet points.')\n",
    "])\n",
    "\n",
    "llm_reviewer = ChatOpenAI(model='gpt-4o-mini', temperature=0.2)\n",
    "\n",
    "review_chain = prompt_reviewer | llm_reviewer | output_parser\n",
    "\n",
    "feedback_output = review_chain.invoke({'report': output})\n",
    "\n",
    "Markdown(feedback_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Understanding Transformers: A Non-AI Researcher’s Guide\n",
       "\n",
       "## Introduction\n",
       "Transformers are a groundbreaking model architecture that has transformed the landscape of artificial intelligence (AI), particularly in natural language processing (NLP). Introduced in the seminal 2017 paper \"Attention is All You Need\" by Vaswani et al., transformers have become the foundation for many cutting-edge AI applications, including language translation, text generation, and more. This report aims to explain how transformers work in a straightforward manner, making it accessible for non-AI researchers.\n",
       "\n",
       "## 1. The Basics of Transformers\n",
       "\n",
       "### 1.1 What is a Transformer?\n",
       "A transformer is a neural network architecture designed to process sequential data, such as text. Unlike previous models that processed data in a linear order (like recurrent neural networks), transformers can analyze all parts of the input simultaneously. This parallel processing capability allows for greater efficiency and effectiveness, akin to reading an entire paragraph at once rather than word by word.\n",
       "\n",
       "### 1.2 Key Components\n",
       "Transformers consist of several key components:\n",
       "- **Input Embeddings**: Words or tokens are converted into numerical vectors that represent their meanings. Think of this as translating words into a language that the model can understand.\n",
       "- **Positional Encoding**: Since transformers do not process data sequentially, positional encoding is added to the input embeddings to provide information about the order of the words. This is similar to adding timestamps to a series of events to understand their sequence.\n",
       "- **Attention Mechanism**: This is the core innovation of transformers, allowing the model to focus on different parts of the input when making predictions. Imagine a teacher highlighting important sections of a textbook while preparing for a lecture.\n",
       "\n",
       "## 2. The Attention Mechanism\n",
       "\n",
       "### 2.1 What is Attention?\n",
       "The attention mechanism enables the model to weigh the importance of different words in a sentence when making predictions. For instance, in the sentence \"The cat sat on the mat,\" the model can learn to focus more on \"cat\" and \"mat\" when predicting the next word, much like a reader emphasizing key terms while summarizing a text.\n",
       "\n",
       "### 2.2 How Attention Works\n",
       "- **Query, Key, and Value**: Each word is transformed into three vectors: a query, a key, and a value. The query represents what the model is looking for, the key represents the information available, and the value is the actual information.\n",
       "- **Calculating Attention Scores**: The model calculates a score for how much focus to place on each word by taking the dot product of the query and key vectors. This score is then normalized using a softmax function to create a probability distribution, akin to assigning weights to different pieces of information based on their relevance.\n",
       "- **Weighted Sum**: Finally, the model computes a weighted sum of the value vectors based on the attention scores, allowing it to concentrate on the most relevant words, similar to how a researcher prioritizes sources when writing a paper.\n",
       "\n",
       "## 3. The Transformer Architecture\n",
       "\n",
       "### 3.1 Encoder and Decoder\n",
       "Transformers are typically composed of two main parts: the encoder and the decoder.\n",
       "- **Encoder**: The encoder processes the input data and generates a set of attention-based representations. It can be thought of as a translator that understands the source language.\n",
       "- **Decoder**: The decoder takes these representations and generates the output, such as a translated sentence, functioning like a translator that produces the target language.\n",
       "\n",
       "### 3.2 Stacking Layers\n",
       "Both the encoder and decoder consist of multiple layers (often 6 to 12). Each layer contains:\n",
       "- **Multi-Head Attention**: This allows the model to focus on different parts of the input simultaneously, similar to having multiple experts analyze various aspects of a problem.\n",
       "- **Feed-Forward Neural Networks**: These networks process the output from the attention mechanism, akin to synthesizing insights from the analysis.\n",
       "- **Residual Connections and Layer Normalization**: These techniques help stabilize training and improve performance, ensuring that the model learns effectively without losing important information.\n",
       "\n",
       "## 4. Training Transformers\n",
       "\n",
       "### 4.1 Data Preparation\n",
       "Transformers require large datasets for training. Text data is typically preprocessed to remove noise and convert words into tokens, much like cleaning and organizing data before analysis.\n",
       "\n",
       "### 4.2 Loss Function\n",
       "During training, the model's predictions are compared to the actual outputs using a loss function, which quantifies the difference. The goal is to minimize this loss through optimization techniques like gradient descent, similar to refining a thesis based on feedback.\n",
       "\n",
       "### 4.3 Transfer Learning\n",
       "Once trained, transformers can be fine-tuned on specific tasks with smaller datasets, making them versatile for various applications. This is akin to a professional adapting their skills to a new job role.\n",
       "\n",
       "## 5. Applications of Transformers\n",
       "\n",
       "Transformers have a wide range of applications, including:\n",
       "- **Natural Language Processing**: Language translation (e.g., Google Translate), sentiment analysis (e.g., determining the emotional tone of a review), and text summarization (e.g., generating concise summaries of articles).\n",
       "- **Computer Vision**: Image classification (e.g., identifying objects in photos) and object detection (e.g., recognizing faces in images).\n",
       "- **Speech Recognition**: Converting spoken language into text (e.g., virtual assistants like Siri and Alexa).\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Transformers represent a significant advancement in AI, particularly in how machines understand and generate human language. By leveraging the attention mechanism and a unique architecture, transformers can process information more effectively than previous models. Understanding the basics of transformers provides valuable insights into the capabilities and potential applications of AI technologies across various fields. As research continues to evolve, future developments may include more efficient training methods, enhanced interpretability of models, and broader applications in areas such as healthcare and education. The ongoing evolution of transformers is likely to shape the future of AI, making it an exciting area for further exploration."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_WRITER_SYS_MSG = \"\"\"\n",
    "You take in a research report and a set of bullet points with feedback to improve,\n",
    "and you revise the research report based on the feedback and write a final version.\n",
    "\"\"\"\n",
    "\n",
    "prompt_final_writer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', FINAL_WRITER_SYS_MSG),\n",
    "        ('human', 'Write a reviewed and improved version of this research report:\\n\\n{report}, based on this feedback:\\n\\n{feedback}.')\n",
    "    ]\n",
    ")\n",
    "llm_final_writer = ChatOpenAI(model='gpt-4o-mini', temperature=0.2)\n",
    "chain_final_writer = prompt_final_writer | llm_final_writer | output_parser\n",
    "\n",
    "output_final_report = chain_final_writer.invoke({'report': output, 'feedback': feedback_output})\n",
    "\n",
    "Markdown(output_final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidhya-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
